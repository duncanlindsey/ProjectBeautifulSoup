,Module,CS & ML,ML,Type,Code_left,Link,Aims,Assessment,Code_right,Content,Learning Outcomes,Method of Instruction,Prerequisites,Reading List,Resources,Taught By,Term,Year
0,Supervised Learning,True,True,Core,COMPGI01,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi01_supervised_learning/,This module covers supervised approaches to machine learning.,"The course has the following assessment components:. - Written Examination (2.5 hours, 75%). - Coursework Section (25%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined. For full details of coursework see the course web page",COMPGI01 (Also taught as: COMPM055 Supervised Learning),"The course consists of both foundational topics for supervised learning such as Linear Regression, Nearest Neighbors and Kernelisation as well contemporary research areas such as multi-task learning and optimisation via proximal methods. In a given year topics will be drawn non-exclusively from the following. - Nearest Neighbors. - Linear Regression. - Kernels and Regularisation. - Support Vector Machines. - Gaussian Processes. - Decision Trees. - Ensemble Learning. - Sparsity Methods. - Multi-task Learning. - Proximal Methods. - Semi-supervised Learning. - Neural Networks. - Matrix Factorization. - Online Learning. - Statistical Learning Theory","Gain in-depth familiarity with various classical and contemporary supervised learning algorithms, understand the underlying limitations and principles that govern learning algorithms and ways of assessing and improving their performance.",Lecture presentations with associated class problems,"Basic mathematics, Calculus, Probability, Linear algebra",http://readinglists.ucl.ac.uk/modules/compgi01.html,Reading list available via the UCL Library catalogue,"Mark Herbster (50%), John Shawe-Taylor (30%), Massi Pontil (20%)",1,MSc
1,Statistical Modelling and Data Analysis,True,False,Core,,,,,,,,,,,,,,
2,Graphical Models,True,True,Group One,COMPGI08,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi08_graphical_models/,To give an introduction to probabilistic modelling covering the broad theoretical landscape. The course aims to cover much of the first 12 chapters of the course textbook www.cs.ucl.ac.uk/staff/d.barber/brml/ The emphasis is on probabilistic modelling of discrete variables.,"The course has the following assessment components:. - Written Examination (2.5 hours, 75%). - Coursework (25%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined. Students will also be required to completed 8 weekly example problems from the course textbook",COMPGI08 (Also taught as COMPM056 ),Bayesian Reasoning. Bayesian Networks. Directed and Undirected Graphical Models. Inference in Singly-Connected Graphs. Hidden Markov Models. Junction Tree Algorithm. Decision Making under uncertainty. Markov Decision Processes. Learning with Missing Data. Approximate Inference using Sampling. If time permits we will also cover some deterministic approximate inference,"Ability to construct probabilistic models, learn parameters and perform inference. This forms the foundation of many models in the wider sciences and students should be able to develop novel models for applications in a variety of related areas.",Lectures,"Excellent understanding and abilities with Linear Algebra, Multivariate Calculus and Probability all at first year undergraduate mathematics level. Assignments will require students to have familiarity with coding a high level language; some tools in Matlab and Julia are provided.",http://readinglists.ucl.ac.uk/modules/compgi08.html,Reading list available via the UCL Library catalogue,Dmitry Adamsky [Teaching Fellow] (100%) David Barber [Module Lead],1,MSc
3,Probabilistic and Unsupervised Learning,True,True,Group One,COMPGI18,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi18_probabilistic_and_unsupervised_learning/,"This course provides students with an in-depth introduction to statistical modelling and unsupervised learning techniques It presents probabilistic approaches to modelling and their relation to coding theory and Bayesian statistics. A variety of latent variable models will be covered including mixture models (used for clustering), dimensionality reduction methods, time series models such as hidden Markov models which are used in speech recognition and bioinformatics, independent components analysis, hierarchical models, and nonlinear models. The course will present the foundations of probabilistic graphical models (e.g. Bayesian networks and Markov networks) as an overarching framework for unsupervised modelling. We will cover Markov chain Monte Carlo sampling methods and variational approximations for inference. Time permitting, students will also learn about other topics in probabilistic (or Bayesian) machine learning.","The course has the following assessment components:. - Written Examination (2.5 hours, 50%). - Examination rubric: Answer all questions. - Coursework Section (3 pieces, 50%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined",COMPGI18,"Basics of Bayesian learning and regression. Latent variable models, including mixture models and factor models. The Expectation-Maximisation (EM) algorithm. Time series, including hidden Markov models and state-space models. Spectral learning. Graphical representations of probabilistic models. Belief propagation, junction trees and message passing. Model selection, hyperparameter optimisation and Gaussian-process regression",To be able to understand the theory of unsupervised learning systems; to have in-depth knowledge of the main models used in Unsupervised Learning; to understand the methods of exact and approximate inference in probabilistic models; to be able to recognise which models are appropriate for different real-world applications of machine learning methods.,Lecture presentations with associated class problems,"A good background in statistics, calculus, linear algebra, and computer science. You should thoroughly review the maths in the cribsheet provided at www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008/cribsheet.pdf before the start of the module. You must either know Matlab or Octave, be taking a class on Matlab/Octave, or be willing to learn it on your own. Any student or researcher at UCL meeting these requirements is welcome to attend the lectures. Students wishing to take it for credit should consult with the module lecturer.",http://readinglists.ucl.ac.uk/modules/compgi18.html,Reading list available via the UCL Library catalogue,Maneesh Sahani (Gatsby Computational Neuroscience Unit) 			 (100%),1,MSc
4,Advanced Deep Learning and Reinforcement Learning,True,True,Group Two,COMPGI22,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi22_advanced_deep_learning_and_reinforcement_learning/,Students successfully completing the module should understand: The basics of deep learning and reinforcement learning paradigms Architectures and optimization methods for deep neural network training How to implement deep learning methods within TensorFlow and apply them to data The theoretical foundations and algorithms of reinforcement learning How to apply reinforcement learning algorithms to environments with complex dynamics,"The course has the following assessment components:. - Coursework (100%). - Deep learning Programming and experimentation in Python/TensorFlow. - Reinforcement Learning Programming and experimentation in Python/TensorFlow. To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined",COMPGI22 (Also taught as COMPMI22 ),"The course has two interleaved parts that converge towards the end of the course. One part is on machine learning with deep neural networks, the other part is about prediction and control using reinforcement learning. The two strands come together when we discuss deep reinforcement learning, where deep neural networks are trained as function approximators in a reinforcement learning setting. The deep learning stream of the course will cover a short introduction to neural networks and supervised learning with TensorFlow, followed by lectures on convolutional neural networks, recurrent neural networks, end-to-end and energy-based learning, optimization methods, unsupervised learning as well as attention and memory. Possible applications areas to be discussed include object recognition and natural language processing. The reinforcement learning stream will cover Markov decision processes, planning by dynamic programming, model-free prediction and control, value function approximation, policy gradient methods, integration of learning and planning, and the exploration/exploitation dilemma. Possible applications to be discussed include learning to play classic board games as well as video games","To understand the foundations of deep learning, reinforcement learning, and deep reinforcement learning including the ability to successfully implement, apply and test relevant learning algorithms in TensorFlow.","Lectures, reading, and course work assignments. Course work will focus on the practical implementation of deep neural network training and reinforcement learning algorithms and architectures in Tensorflow","The prerequisites are probability, calculus, linear algebra AND COMPGI01 Supervised Learning OR COMPGI08 Graphical Models OR COMPGI18 Probabilistic and Unsupervised Learning . In order to successfully complete the coursework for this module, students will require excellent coding skills in Python.",http://readinglists.ucl.ac.uk/modules/compmi22.html,Reading list available via the UCL Library catalogue,Thore Graepel (50%) Hado van Hasselt (50%) The course is taught in collaboration with DeepMind. The majority of lectures will be taught by guest lecturers from DeepMind who are leading experts in the field of machine learning and will teach about topics in which they are specialised.,2,MSc
5,Advanced Topics in Machine Learning,True,True,Group Two,COMPGI13,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi13_advanced_topics_in_machine_learning/,"Kernel methods To gain an understanding of the theory and applications of kernel methods, including: An overview of how kernel feature spaces can be constructed, including in infinite dimensions, and the smoothing properties of functions in these spaces. Simple and complex learning algorithms using kernels (ridge regression, kernel PCA, the support vector machine) Representations of probabilities in reproducing kernel Hilbert spaces. Statistical two-sample and independence tests, and learning algorithms using these embeddings (clustering, ICA) Learning theory To learn the fundamentals of statistical learning theory. In particular to: Understand what characterizes a learning problem and what it means for an algorithm/system/machine to “learn”. Understand the key role of regularization and the different approaches to use it efficiently in practice. Acquire familiarity with a variety of statistically consistent learning algorithms, both from modeling and practical perspectives.",The course has the following assessment components:. - Written Examination (50%). - Coursework Section (50%),COMPGI13 (Also taught as: COMPM050 );,"Introduction to kernel methods:. - Definition of a kernel, how it relates to a feature space, The reproducing kernel Hilbert space. - Simple applications: kernel PCA, kernel ridge regression. - Distance between means in RKHS, integral probability metrics, the maximum mean discrepancy (MMD), two-sample tests. - Choice of kernels for distinguishing distributions, characteristic kernels. - Covariance operator in RKHS: proof of existence, definition of norms (including HSIC, the Hilbert-Schmidt independence criterion). - Application of HSIC to independence testing. - Feature selection, taxonomy discovery. - Introduction to independent component analysis, kernel ICA. - Large margin classification, support vector machines for classification. Introduction to supervised learning in the context of statistical learning theory:. - a taxonomy of learning problems. - no free lunch theorem. - regularization. - model selection. - stability and generalization. - measures of complexity for hypotheses spaces. - sample complexity, generalization bounds","To gain in-depth familiarity with the selected research topics, understand how to design and implement learning algorithms. To be able to individually read, understand and discuss research papers in the field.",Frontal teaching using whiteboard and slides,"Linear Algebra, Probability Theory, Calculus",http://readinglists.ucl.ac.uk/modules/compgi13.html,Reading list available via the UCL Library catalogue,Arthur Gretton (50%) and Carlo Ciliberto (50%),1,MSc
6,Applied Machine Learning,True,True,Group Two,COMPGI09,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi09_applied_machine_learning/,To give a detailed understanding of topics related to efficient implementation of large-scale machine learning with a focus on optimisation in both linear and non-linear machine learning models. Students will also gain experience in tackling real world problems through solving online machine learning challenges. A key aim is that students understand the challenges of optimisation and associated time and space complexities of various approaches.,"The course has the following assessment components:. - Written Examination (2.5 hours, 75%). - Coursework Section. The coursework is based on assessed practical challenges hosted by Kaggle (25%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined",COMPGI09 (also taught as COMPM090 ),"First Order Optimisation methods (gradient descent). Second Order Optimisation methods (Newton and Quasi Newton approaches and Conjugate Gradients). Methods for solving Large Scale Linear, including Conjugate Gradients. Automatic Differentiation methods for efficiently computing first and second order gradients. Classical methods for Regression and Classification including linear and logistic regression. Methods for Unsupervised Learning including mixture modelling. Deep Learning Methods for Regression, Classification and Unsupervised Learning. Recurrent Networks for Time-Series processing. Matrix and Tensor Factorisation. Visualisation methods including Autoencoders and tSNE","Students will have a good understanding of a variety of optimisation methods applicable for large-scale machine learning, including first and second order methods and automatic differentiation. Students will also become familiar with techniques used in practice to solve real world machine learning problems.",Lectures (3 hours per week),"Please note that this is not an introduction to machine learning . Students are required to have an excellent understanding and abilities with Linear Algebra, Multivariate Calculus and Probability all at first year undergraduate mathematics level. Assignments will require students to have familiarity with coding a high level language. It is strongly recommend that students are skilled in Python. Students should have taken in term 1 introductory modules in machine learning, either Introduction to Machine Learning , Introduction to Data Science or Supervised Learning . Ideally students will have taken Graphical Models or Unsupervised Learning .",http://readinglists.ucl.ac.uk/modules/compgi09.html,Reading list available via the UCL Library catalogue,Dimitry Adamsky [Teaching Fellow] (100%) David Barber [Module Lead],2,MSc
7,Approximate Inference and Learning in Probabilistic Models,True,True,Group Two,COMPGI16,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi16_approximate_inference_and_learning_in_probabilistic_models/,"The module will present the foundations of approximate inference and learning in probabilistic graphical models (e.g. Bayesian networks and Markov networks), with particular focus on models composed from conditional exponential family distributions. Both stochastic (Monte Carlo) methods and deterministic approximations will be covered. The methods will be discussed in relation to practical problems in real-world inference in Machine Learning, including problems in tracking and learning.","The course has the following assessment components:. - Written Examination (2.5 hours, 50%). - Examination rubric: Answer all questions. - Coursework Section (3 pieces, 50%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined",COMPGI16,"Nonlinear, hierarchical (deep), and distributed models. Independent component analysis, Boltzmann machines, Dirichlet topic models, manifold discovery. Mean-field methods, variational approximations and variational Bayes. Expectation propagation. Loopy belief propagation, the Bethe free energy and extensions. Convex methods and convexified bounds. Monte-Carlo methods: including rejection and importance sampling, Gibbs, Metropolis-Hastings, anealed importance sampling, Hamiltonian Monte-Carlo, slice sampling, sequential Monte-Carlo (particle filtering). Other topics as time permits",Students will be able to understand how to derive and implement state-of-the-art approximate inference techniques and be able to make contributions to research in this area.,Lecture presentations with associated class problems,COMPGI18 Probabilstic and Unsupervised Learning,http://readinglists.ucl.ac.uk/modules/compgi16.html,Reading list available via the UCL Library catalogue,Maneesh Sahani (Gatsby Computational Neuroscience Unit) 			 (100%),1,MSc
8,Information Retrieval & Data Mining,True,True,Group Two,COMPGI15,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi15_information_retrieval_data_mining/,"The course is aimed at an entry level study of information retrieval and data mining techniques. It is about how to find relevant information and subsequently extract meaningful patterns out of it. While the basic theories and mathematical models of information retrieval and data mining are covered, the course is primarily focused on practical algorithms of textual document indexing, relevance ranking, web usage mining, text analytics, as well as their performance evaluations. Practical retrieval and data mining applications such as web search engines, personalisation and recommender systems, business intelligence, and fraud detection will also be covered.","The course has the following assessment components:. . - Written Examination (2.5 hours, 60%). - Coursework (40%). . To pass this course, students must:. . - Obtain an overall pass mark of 50% for all sections combined",COMPGI15 (Also taught as COMPM052 ),"Overview of the fields Study some basic concepts of information retrieval and data mining, such as the concept of relevance, association rules, and knowledge discovery. Understand the conceptual models of an information retrieval and knowledge discovery system. Indexing Introduce various indexing techniques for textual information items, such as inverted indices, tokenization, stemming and stop words. Retrieval Methods Study popular retrieval models: 1 Boolean, 2. Vector space, 3 Binary independence, 4 Language modelling. Probability ranking principle. Other commonly-used techniques include relevance feedback, pseudo relevance feedback, and query expansion. Evaluation of Retrieval Performance. . Measurements Average precision, NDCG, etc. ""Cranfield paradigm"" and TREC conferences. Personalisation and Usage Mining Study basic techniques for collaborative filtering and recommender systems, such as the memory-based approaches, probabilistic latent semantic analysis (PLSA), personalized web search through click-through data. Data Mining Study basic techniques, algorithms, and systems of data mining and analytics, including frequentpattern and correlation and association analysis, anomaly detection, and click-through modelling. Emerging Areas MapReduce and Sparck; Learning to Rank; Portfolio retrieval and Risk Management; Deep Learning","Students are expected to master both the theoretical and practical aspects of information retrieval and data mining. At the end of the course student are expected to understand 1. The common algorithms and techniques for information retrieval (document indexing and retrieval, query processing, etc). 2. The quantitative evaluation methods for the IR systems and data mining techniques. 3. The popular probabilistic retrieval methods and ranking principles. 4. The techniques and algorithms existing in practical retrieval and data mining systems such as those in web search engines and recommender systems, including the recently popular topic of deep learning. 5. The challenges and existing techniques for the emerging topics of MapReduce, portfolio retrieval and deep learning.","Lecture presentations, Practical exercises","Normally offered only to students in computer science related programmes because basic programming skills are required. Basic understanding of probability and statistics and proficient in java programming, as demonstrated by a least one programing project in the past.",http://readinglists.ucl.ac.uk/modules/compgi15.html,Reading list available via the UCL Library catalogue,Emine Yilmaz (50%) Ingemar Cox (50%),1,MSc
9,Introduction to Deep Learning,True,True,Group Two,COMPGI23,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi23_introduction_to_deep_learning/,To have a full understanding of the learning outcomes.,"The course has the following assessment components:. - Coursework (100%) [comprises 3 assignments of 20%, 20% and 60%]. To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined",COMPGI23 ( Also taught as COMPM089 ),"This module will aim to teach students the fundamentals of modern multi-layered neural networks. It will cover the most common forms of model architectures and primarily the algorithms used to train them. The theory and principles will be presented, but this will go hand-in-hand with a focus on practical experience such as using existing frameworks and implementing (simplified versions) of core algorithms. Students will be taught the basics of neural networks, convolutional networks, recurrent networks; and introduced to concepts such as: dropout, batch normalization, types of hyper-parameter optimization, distributed and constrained computing variants. Applications in the area of audio processing and image captioning and vision will be discussed","At the conclusion of this module students should understand: The fundamental principles, theory and approaches for learning with deep neural networks The main variants of deep learning (such convolutional and recurrent architectures), and their typical applications The key concepts, issues and practices when training and modeling with deep architectures; as well as have hands-on experience in using deep learning frameworks for this purpose How to implement basic versions of some of the core deep network algorithms (such as backpropagation) How deep learning fits within the context of other ML approaches and what learning tasks it is considered to be suited and not well suited to perform",Lectures,N/A,http://readinglists.ucl.ac.uk/modules/compm089.html,Reading list available via the UCL Library catalogue,Nic Lane (100%),1,MSc
10,Machine Vision,True,True,Group Two,COMPGI14,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi14_machine_vision/,"The course addresses algorithms for automated computer vision. It focuses on building mathematical models of images and objects and using these to perform inference. Students will learn how to use these models to automatically find, segment and track objects in scenes, perform face recognition and build three-dimensional models from images.","The course has the following assessment components:. - Written Examination (2.5 hours, 80%). - Coursework Section (2 pieces, 20%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined. The examination rubric is: Answer 3 questions",COMPGI14 (Also taught as COMPM054 ),"Two-dimensional visual geometry: 2d transformation family. The homography. Estimating 2d transformations. Image panoramas. Three dimensional image geometry: The projective camera. Camera calibration. Recovering pose to a plane. More than one camera: The fundamental and essential matrices. Sparse stereo methods. Rectification. Building 3D models. Shape from sillhouette. Vision at a single pixel: background subtraction and color segmentations problems. Parametric, non-parametric and semi-parametric techniques. Fitting models with hidden variables. Connecting pixels: Dynamic programming for stereo vision. Markov random fields. MCMC methods. Graph cuts. Texture: Texture synthesis, super-resolution and denoising, image inpainting. The epitome of an image. Dense Object Recognition: Modelling covariances of pixel regions. Factor analysis and principle components analysis. Sparse Object Recognition: Bag of words, latent dirilecht allocation, probabilistic latent semantic analysis. Face Recognition: Probabilistic approaches to identity recognition. Face recognition in disparate viewing conditions. Shape Analysis: Point distribution models, active shape models, active appearance models. Tracking: The Kalman filter, the Condensation algorithm","To be able to understand and apply a series of probabilistic models of images and objects in machine vision systems. To understand the principles behind face recognition, segmentation, image parsing, super-resolution, object recognition, tracking and 3D model building.","Lectures, practical lab classes","Successful completion of an appropriate Computer Science, Mathematics, or other Physical Science or Engineering undergraduate programme with sufficient mathematical and programming content, plus some familiarity with digital imaging and digital image processing.",http://readinglists.ucl.ac.uk/modules/compgi14.html,Reading list available via the UCL Library catalogue,Gabriel Brostow(100%),1,MSc
11,Statistical Natural Language Processing,True,True,Group Two,COMPGI19,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi19_statistical_natural_language_processing/,The course introduces the basics of statistical natural language processing (NLP) including both linguistics concepts such as morphology and syntax and machine learning techniques relevant for NLP.,The course has the following assessment component:. - Coursework (100%). To pass this module students must:. - Obtain an overall pass mark of 50% for all sections combined,COMPGI19 (also taught as COMPM083 ),"NLP is domain-centred fields, as opposed to technique centred fields such as ML, and as such there is no ""theory of NLP"" which can be taught in a cumulative technique-centred way. Instead this course will focus on one or two NLP end-to-end ""pipelines"" (such as Machine Translation and Machine Reading). Through these applications the participants will learn about language itself, relevant linguistic concepts, and Machine Learning techniques. For the latter an emphasis will be on structured prediction, a branch of ML that is particularly relevant to NLP. Topics will include (but are not restricted to) machine translation, sequence tagging, constituent and dependency parsing, information extraction, semantics.  The course has a strong applied character, with coursework to be programmed, and lab classes to teach students to write software that processes language. NLP Tasks. - Language Models. - Machine Translation. - Text Classification. - Sequence Tagging. - Constituency Parsing. - Dependency Parsing. - Information Extraction. - Machine Comprehension. NLP and ML methods. - Structured Prediction. - Generative Learning. - Smoothing. - EM Algorithm. - Discriminative Learning. - Deep and Representation Learning","Students successfully completing the module should understand: relevant linguistic concepts relevant ML techniques, in particular structured prediction what makes NLP challenging (and exciting) how to write programs that process language how to rigorously formulate NLP tasks as learning and inference tasks, and address the computational challenges involved.","The module is delivered in lectures, with occasional guest lectures by leading researchers in NLP. Coursework problems will focus on basic components in an NLP pipeline, such as a document classifier, part-of-speech tagger and syntactic parser. The module is based on the lecture notes, exercises and slides at github.com/uclmr/stat-nlp-book , our own online book with interactive notes and slides, based on Python and Jupyter Notebooks",Be able to write code in Python. Understand Basic Probability Theory (e.g. Bayes Rule) and Linear Algebra. Be able to install libraries on a computer.,http://readinglists.ucl.ac.uk/modules/compgi19.html,Reading list available via the UCL Library catalogue,Sebastian Riedel (100%),1,MSc
12,Applied Bayesian Methods,True,False,Group Three,,,,,,,,,,,,,,
13,Statistical Design of Investigations,True,False,Group Three,,,,,,,,,,,,,,
14,Statistical Inference,True,False,Group Three,,,,,,,,,,,,,,
15,Affective Computing and Human-Robot Interaction,False,True,Group Two,COMPGI17,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi17_affective_computing_and_human_robot_interaction/,"The module targets students who have no previous knowledge in cognitive science and emotion theory and therefore the aim of Part 1 of the module is to give a basic introduction to the theory of emotion from psychology and neuroscience viewpoints and to understand its importance in human decision and communication processes Part 2 will concentrate on the application of machine learning techniques to emotion recognition by looking at current applications in entertainment, education, and health. Part 3 will focus on the challenges in designing robots that are capable of socially interacting with humans. Examples of current applications in entertainment, education, health, therapy, rehabilitation, service robotics, rescue robots will be used to identify problems and discuss machine learning solutions for the topics taught in Parts 2 and 3.","The course has the following assessment components:. - Coursework Section (1 piece, 40%). - Written Examination (2.5 hours, 60%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined",COMPGI17 (Also taught as COMPM082 ),"Emotion theory. - What is affect, emotion, mood?. - Why do we have emotions?. - Neurological and psychological perspectives. - How do humans express and recognise emotions?. - Emotion expression models, appraisal and causal theories. - Affective and social interaction. Affective computing. - Affective computing. - definition. - aims. Emotion Recognition. - Application of machine learning techniques for adaptive emotion recognition from single modality e.g. - facial expressions. - biosignals. - Adaptive multimodal emotion recognition: signal fusion. Human-Robot Interaction (HRI). - Social robotics: motivation and emotions in robots. - Emotion based architecture. - Evaluation methods for HRI research. - Ethical issues in Affective Computing and HRI research","To have a basic knowledge of emotion models and of how technology (e.g., robot) can be endowed with the ability to affectively and socially interact with its user. To understand the challenges that affective computing and HRI pose to the machine learning field and identify the advantages and disadvantages of different machine learning techniques to address those issues. To understand how traditional HCI methods need to be modified to be applied to the HRI field.","Lecture presentations, programming assignments","Fundamentals of calculus, probability, statistics or have taken either COMPGI01 Supervised Learning or COMPGI21 Introduction to Machine Learning in term 1.",http://readinglists.ucl.ac.uk/modules/compgi17.html,Reading list available via the UCL Library catalogue,Nadia Berthouze (100%),2,MSc
16,Bioinformatics,False,True,Group Two,COMPGI10,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi10_bioinformatics/,"The overall aim of this course is to introduce students to the new field of bioinformatics (computational biology) and how machine learning techniques can be employed in this area. The course is aimed at students who have no previous knowledge of biology and so the aim of Part 1 of the course is to give a basic introduction to molecular biology as a background for bioinformatics. Part 2 will concentrate on modern bioinformatics applications, particularly those which make good use of pattern recognition and machine learning methods.","The course has the following assessment components:. - Written Examination (2.5 hours, 85%). - Coursework Section (1 individual mini-project, 15%). To pass this module, students must:. - Obtain an overall pass mark of 50% for all components combined",COMPGI10 (Also taught as COMPM058 ),"Part 1: Basic molecular biology (6 lectures). - Introduction to Basic Cell Chemistry: Cell chemistry and macromolecules. Biochemical pathways e.g. Glycolysis. Protein structure and functions. - Cell Structure and Function: Cell components. Different types of cell. Chromosome structure and organisation. Cell division. - The Hereditary Material: DNA structure, replication and protein synthesis. Structure and roles of RNA. Genetic code. Mechanism of protein synthesis: transcription and translation. Mutation. - Recombinant DNA Technology: Restriction enzymes. Hybridisation techniques. Gene cloning. Polymerase chain reaction. - Genomics and Structural Genomics: Genes, genomes, mapping and DNA sequencing. Part 2: Bioinformatics Applications (3 lectures per subject). - Biological Databases: Overview of the use and maintenance of different databases in common use in biology. - Case study: the CATH database of protein structure. - Gene Prediction: Methods for analysing genomic DNA to identify genes. Techniques: neural networks and HMMs. - Detecting Distant Homology: Methods for inferring remote relationships between genes and proteins. Techniques: dynamic programming, HMMs, hierachical clustering. - Protein Structure Prediction: Methods for predicting the secondary and tertiary structure of proteins. Techniques: neural networks, SVMs, genetic algorithms and stochastic global optimization. - Transcriptomics: Methods for analysing gene expression and microarray data. Techniques: clustering, SVMs. - Agent-based Genome Analysis: Automation of genome analysis using intelligent software agents. - Drug Discovery Informatics: Approaches to drug discovery using bioinformatics techniques",To have a basic knowledge of modern molecular biology and genomics. To understand the advantages and disadvantages of different machine learning techniques in bioinformatics and how the relative merits of different approaches can be evaluated by correct benchmarking techniques. To understand how theoretical approaches can be used to model and analyse complex biological systems.,Lecture presentations with associated class problems and group presentation/discussion of key research papers,"It is expected that students will already be familiar with the principles of techniques such as neural networks, Support Vector Machines, Hidden Markov Models from earlier parts of the course.",http://readinglists.ucl.ac.uk/modules/compgi10.html,Reading list available via the UCL Library catalogue,David Jones (66%) Kevin Bryson (33%),2,MSc
17,Programming & Mathematical Methods for Machine Learning,False,True,Group Two,COMPGI07,http://www.cs.ucl.ac.uk/current_students/syllabus/compgi/compgi07_programming_mathematical_methods_for_machine_learning/,"The overall aim of this course is to introduce or refresh Matlab programming, computational complexity and linear algebra with an aim to its application to machine learning.","The course has the following assessment components:. - Coursework (100%). To pass this course, students must:. - Obtain an overall pass mark of 50% for all sections combined. For full details see the course web page",COMPGI07 (Also taught as: COMPM012),"Part A: An introduction to Matlab for beginning programmers. Matlab Interface Matrix Operations Control flow I/O Plotting. Part B: An introduction to computational complexity. Three paradigms of algorithm design (greedy, divide & conquer, dynamic pro- gramming) Time complexity of algorithms and problems Turing machines and Decision Problems Unsolvable problems Intractability (NP-completeness). Part C: Elements of linear algebra and its applications. Vector and matrix operations, Orthogonality, Norms, Singular Value Decomposition (SVD). Applications of SVD to Machine Learning and data analysis. Principal component analysis, Least squares and regularization, Kernel Methods. Spectral graph theory","Students successfully completing the module should be able to program machine learning algorithms in matlab, analyse the complexity of algorithms, and become confident with vector/matrix computations and basic concepts of linear algebra which are important for the development of machine learning algorithms.",Part A: Labs with intensive programming coursework which is oriented towards machine learning. Part B: Lecture presentations. Part C: Lecture presentations with associated class problems,"None, but student must also be enrolled in GI01",http://readinglists.ucl.ac.uk/modules/compgi07.html,Reading list available via the UCL Library catalogue,"Mark Herbster (Part A,B) (50%), Massi Pontil (Part C) (50%)",1,MSc
